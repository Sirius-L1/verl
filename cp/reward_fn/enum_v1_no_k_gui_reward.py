import logging

try:
    # Change from relative imports to absolute imports
    from cp.reward_fn.reward import point_enum_v1_no_k as point_enum_reward
except ImportError as e:
    logging.error(f"Error importing reward modules: {e}. Ensure __init__.py files exist and relative paths are correct.")
    raise e

FMT_RATIO = 0.0
ACC_RATIO = 1.0

# Dictionary to map data_source to the respective reward calculation function
# The functions in the modules should all have the same name, e.g., 'calculate_reward'
REWARD_HANDLERS = {
    "point": point_enum_reward.calculate_reward,
    # TODO: Add more handlers as you create them
    # "new_type": new_type_reward_module.calculate_reward,
    
    # ========= validation set reward ========
    "ss2": point_enum_reward.calculate_reward,
    "ssp": point_enum_reward.calculate_reward,
    "osg": point_enum_reward.calculate_reward,
}

def enum_v1_no_k_gui_reward_function(data_source, solution_str, ground_truth, extra_info=None, **kwargs):
    """
    Main reward function dispatcher for the 'cyber' project.
    Delegates reward calculation to specific functions based on the data_source using a dictionary lookup.

    Args:
        data_source (str): The source or type of the data (e.g., "point", "bbox").
        solution_str (str): The solution string generated by the model.
        ground_truth (any): The ground truth data.
        extra_info (dict, optional): Any extra information passed along.
        **kwargs: Additional keyword arguments that might be passed from the PPO trainer config.

    Returns:
        float: The calculated reward score.
    """
    handler = REWARD_HANDLERS.get(data_source, None)

    if handler:
        try:
            return handler(
                solution_str,
                ground_truth,
                extra_info=extra_info,
                fmt_ratio=FMT_RATIO,
                acc_ratio=ACC_RATIO,
                **kwargs
            )
        except Exception as e:
            logging.error(f"Error executing reward handler for data_source '{data_source}': {e}", exc_info=True)
            return {
                "score": 0.0,
                "format": 0.0,
                "accuracy": 0.0,
                "pred": "",
                "num_pred": 0,
                "has_correct": 0,
                "first_correct": 0,
                "only_correct": 0
            }  # Return a default penalty score on error
    else:
        raise ValueError(f"Unknown data_source: '{data_source}'. No specific reward handler defined.")

